{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "T7JF5oK_Qv1l",
    "outputId": "445e8aa9-c6a0-422f-ba5d-0472001811c0"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "import os \n",
    "import pandas as pd\n",
    "import dateutil\n",
    "import sklearn.preprocessing as preprocess\n",
    "import numpy as np\n",
    "import pdb\n",
    "if IN_COLAB:\n",
    "    !pip install nba_api \n",
    "    from nba_api.stats.static import teams\n",
    "    from nba_api.stats.endpoints import leaguegamefinder\n",
    "    import io\n",
    "pd.options.display.max_columns  = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HoKhrDFyQv1t"
   },
   "source": [
    "## Load the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "q-QF0tTvUNNc",
    "outputId": "7e482f4b-cb3f-402a-facb-7e401f1a7758"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "MWkgMwMCQv1u",
    "outputId": "7a6aece8-658d-44b1-f8d3-fd18de26ddd5"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    f = \"cleanNBA.csv\"\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[f]))\n",
    "else:\n",
    "    fn = os.path.join(\"data\", \"cleanNBA.csv\")\n",
    "    df = pd.read_csv(fn)\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WhDlI72Qv12"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zIFxALUQv19"
   },
   "source": [
    "### Remove uncecessary columns and add home vs. away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMcgbP5BQv2E"
   },
   "outputs": [],
   "source": [
    "#need to extract home vs away info\n",
    "def extractHome(df, matchCol, homeCol, awayCol):\n",
    "    df = df.copy(deep=True)\n",
    "    df[homeCol] = 0\n",
    "    df[awayCol] = 0\n",
    "    sym = np.array([s.split()[1] for s in df[matchCol]], copy=False)\n",
    "    homeMask = sym == 'vs.'\n",
    "    df.loc[homeMask, homeCol] = 1\n",
    "    df.loc[np.logical_not(homeMask), awayCol] = 1\n",
    "    return df\n",
    "\n",
    "df_drop = extractHome(df, 'MATCHUP','HOME', 'AWAY')\n",
    "#let's drop some unnecessary columns\n",
    "cols2drop = ['TEAM_NAME','REB','MATCHUP'] \n",
    "#i'll keep some that may be useful later or may just drop them later\n",
    "df_drop = df_drop.drop(cols2drop,axis=1)\n",
    "df_drop.rename({'MIN': 'MINUTES'},axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "e9c_YFCXQv2J",
    "outputId": "330dc8c8-e7d5-448d-8dbc-3844d89478f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_drop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrg8avjvQv2P"
   },
   "source": [
    "### Replace Fine Season ID's with coarser ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3PPu4zOQv2P"
   },
   "outputs": [],
   "source": [
    "def cleanSeasons(df, szIDCol, newSzIDCol, playOffCol, regCol):\n",
    "    \"\"\"SeasonIDs look like _Year where _ can be 1, 2, or 4\n",
    "    It seems like 1 is pre-new year, 2 is post-new-year and 4 is playoffs\n",
    "    We're going to reset seasonID so that a szn can be from Oct-June\n",
    "    and set a playoff flag\n",
    "    \"\"\"\n",
    "    df = df.copy(deep=True)\n",
    "    sznIDs = pd.unique(df[szIDCol])\n",
    "    sznIDsStr = sznIDs.astype(str)\n",
    "    year = [int(ID[1:]) for ID in sznIDsStr]\n",
    "    df[newSzIDCol] = 0 #allocate\n",
    "    df[playOffCol] = 0 #allocate\n",
    "    df[regCol] = 1\n",
    "    for i, oldID in enumerate(sznIDs):\n",
    "        newID = year[i]\n",
    "        mask = df[szIDCol] == oldID\n",
    "        df.loc[mask, newSzIDCol] = newID\n",
    "        if str(oldID)[0] == '4':\n",
    "            #playoffs\n",
    "            df.loc[mask, playOffCol] = 1\n",
    "            df.loc[mask, regCol] = 0\n",
    "    return df\n",
    "df_seas = cleanSeasons(df_drop, 'SEASON_ID','SZSTART', 'PLAYOFF','REG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "i2DjtkZcQv2T",
    "outputId": "8a2e0eae-c3a1-4255-bc32-f04dc6c41235"
   },
   "outputs": [],
   "source": [
    "df_seas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iijQ26WQQv2Y"
   },
   "source": [
    "### Construct time features \n",
    "one hot encode the day of the week, and insert days_since_season_begain, and game_since_season_began (team specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZhE_BBOQv2Y",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eng_timeFeats(df, dateCol='Date', sznIDCol=None, daySznCol=None,\n",
    "                    dayOfWeek=False, teamCol='Team', gSznCol=None):\n",
    "    \"\"\" Engineer time features. \n",
    "    Extract various time features and insert into df.\n",
    "    Features:\n",
    "        day of week \n",
    "        day since season start \n",
    "        that team's game since season start\n",
    "        will convert szn id to numeric index [0, numSzn-1]\n",
    "    Returns\n",
    "        converted df\n",
    "    \"\"\"\n",
    "    ndf = df.copy(deep=True)\n",
    "    ndf[dateCol] = df[dateCol].apply(dateutil.parser.parse)\n",
    "    if dayOfWeek:\n",
    "        dayOfWeek = ndf[dateCol].dt.dayofweek.values\n",
    "        ohe = preprocess.OneHotEncoder(sparse=False,categories='auto')\n",
    "        ohe_dayOfWeek = ohe.fit_transform(dayOfWeek.reshape(-1,1))\n",
    "        day_cols = ['S','M','T','W','Th','F','Sa']\n",
    "        ndf[day_cols] = pd.DataFrame(np.zeros(ohe_dayOfWeek.shape))\n",
    "        ndf[day_cols] = ohe_dayOfWeek\n",
    "    \n",
    "    if daySznCol is not None:\n",
    "        assert sznIDCol is not None, \"Must provide season ID col\"\n",
    "        assert gSznCol is not None, \"May as well do games too\" \n",
    "        ndf.sort_values(by=dateCol, inplace=True)\n",
    "        ndf[daySznCol] = 0 #allocate\n",
    "        ndf[gSznCol] = 0\n",
    "        uniqueSzns = pd.unique(ndf[sznIDCol])\n",
    "        uniqueTeams = pd.unique(ndf[teamCol])\n",
    "        for szID in uniqueSzns:\n",
    "            inSznMask = ndf[sznIDCol] == szID\n",
    "            firstDate = ndf.loc[inSznMask, dateCol].iloc[0]\n",
    "            ndf.loc[inSznMask, daySznCol] = (ndf.loc[inSznMask, dateCol]\n",
    "                                            - firstDate).dt.days.values\n",
    "            for team in uniqueTeams:\n",
    "                inTeamMask = ndf[teamCol] == team\n",
    "                inSzinTeam = inTeamMask & inSznMask\n",
    "                ndf.loc[inSzinTeam, gSznCol] = np.arange(sum(inSzinTeam))\n",
    "                \n",
    "            \n",
    "    return ndf\n",
    "df_t = eng_timeFeats(df_seas,'GAME_DATE','SZSTART','DsinceSzn',\n",
    "                                     True, 'TEAM_ID','GsinceSzn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "DuVA0M7IQv2d",
    "outputId": "21a6ee9a-abba-42b5-f8ec-0a68b11415e6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ia96lhnkQv2i"
   },
   "source": [
    "### One hot encode the teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpQbatFoQv2i"
   },
   "outputs": [],
   "source": [
    "def ohe_teams(df, teamCol):\n",
    "    df = df.copy(deep=True)\n",
    "    teams = df[teamCol].values\n",
    "    ohe = preprocess.OneHotEncoder(sparse=False)\n",
    "    ohe_teams = ohe.fit_transform(teams.reshape(-1,1))\n",
    "    df[ohe.categories_[0]] = pd.DataFrame(np.zeros(ohe_teams.shape)) #workaround to add multiple columns \n",
    "    df[ohe.categories_[0]] = ohe_teams\n",
    "    return df, ohe\n",
    "df_oheT,ohe_encoder = ohe_teams(df_t, 'TEAM_ABBREVIATION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bFK0sKMEQv2m"
   },
   "source": [
    "### Scale and Reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "gQ8GRXzGQv2o",
    "outputId": "c15f6f12-54b9-4cc4-962c-22c3dbb4c6bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numCols = ['MINUTES','PTS','FGM', 'FGA','FG3M','FG3A','FTM','FTA',\n",
    "          'OREB','DREB','AST','STL','BLK','TOV','PF','SZSTART','DsinceSzn','GsinceSzn']\n",
    "team_names = ohe_encoder.categories_[0]\n",
    "oheCols = ['HOME','AWAY', 'PLAYOFF','REG','S','M','T','W','Th','F','Sa'] + team_names.tolist()\n",
    "cols2scale = numCols + oheCols\n",
    "scaler = preprocess.MinMaxScaler()\n",
    "scaler.fit(df_oheT[cols2scale])\n",
    "df_oheT[cols2scale] = scaler.transform(df_oheT[cols2scale])\n",
    "#reorder\n",
    "helperCols = ['SEASON_ID','TEAM_ID','TEAM_ABBREVIATION','GAME_ID','GAME_DATE']\n",
    "new_cols = helperCols + numCols + oheCols\n",
    "clean_df = df_oheT[new_cols]\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7lw_fP9Qv2s"
   },
   "outputs": [],
   "source": [
    "def transScalar(val, col, colsScaled, scaler):\n",
    "    \"\"\"\n",
    "    Utility function to transform a scalar using the scaler.\n",
    "    \"\"\"\n",
    "    idx = colsScaled.index(col)\n",
    "    temp = np.zeros((1,len(colsScaled)))\n",
    "    temp[0, idx] = val\n",
    "    transformed = scaler.transform(temp)\n",
    "    return transformed[0, idx]\n",
    "def transCol(valCol, nameCol, colsScaled, scaler):\n",
    "    \"\"\"\n",
    "    Utility function to transform a column vector using the scaler\n",
    "    \"\"\"\n",
    "    idx = colsScaled.index(nameCol)\n",
    "    valColLen = max(valCol.shape)\n",
    "    temp = np.zeros((valColLen, len(colsScaled)))\n",
    "    temp[:, idx] = valCol\n",
    "    transformed = scaler.transform(temp)\n",
    "    return transformed[:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYKMP3cSQv21"
   },
   "outputs": [],
   "source": [
    "def ridDupCols(df, cols, drop_suff, rn_suff, inplace=True):\n",
    "    \"\"\"\n",
    "    Get rid of duplicate columns caused by merge\n",
    "    \"\"\"\n",
    "    drop_cols = [c + drop_suff for c in cols]\n",
    "    rn_cols = {c + rn_suff : c for c in cols}\n",
    "    if inplace:\n",
    "        df.drop(drop_cols,axis=1, inplace=inplace)\n",
    "        df.rename(columns=rn_cols, inplace=inplace)\n",
    "        return None\n",
    "    else:\n",
    "        df = df.drop(drop_cols,axis=1, inplace=inplace)\n",
    "        df = df.rename(columns=rn_cols, inplace=inplace)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0haPjIUZQv25"
   },
   "source": [
    "### Merge data so one row = one game\n",
    "\n",
    "We will really only be using this as a reference to construct the histories and get the point differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dnvYwO2aQv26"
   },
   "outputs": [],
   "source": [
    "#now would like to merge them based on gameID\n",
    "home_mask= df_t['HOME'] == 1\n",
    "away_mask = np.logical_not(home_mask)\n",
    "\n",
    "home_df = df_t[home_mask]\n",
    "away_df = df_t[away_mask]\n",
    "\n",
    "merge_df = home_df.merge(away_df, 'inner',on='GAME_ID',\n",
    "                         suffixes=('_H','_A'))\n",
    "dup_cols = ['SEASON_ID','GAME_DATE','SZSTART','S','M','T',\n",
    "            'W','Th','F','Sa','DsinceSzn', 'PLAYOFF','REG']\n",
    "ridDupCols(merge_df, dup_cols, drop_suff='_A',rn_suff='_H')\n",
    "merge_df.sort_values(by='GAME_DATE', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "colab_type": "code",
    "id": "JYSpsUYjQv28",
    "outputId": "6bfd5eca-8478-4ce5-af63-02a713582389",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "bN3eRlitQv3A",
    "outputId": "59d4cdb2-4416-478c-b029-51ac0425c212",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#also have to scale the appropiate columns from merged that we will use in currMatchup\n",
    "mergeScaled_df = merge_df.copy()\n",
    "curr_commcols = ['SZSTART','PLAYOFF','REG','S','M','T','W','Th','F','Sa','DsinceSzn']\n",
    "currIcols = ['GsinceSzn']\n",
    "for col in curr_commcols:\n",
    "    if col in cols2scale:\n",
    "        mergeScaled_df[col]= transCol(mergeScaled_df[col].values, col, cols2scale,scaler)\n",
    "for col in currIcols:\n",
    "    for suff in (\"_H\",\"_A\"):\n",
    "        if col in cols2scale:\n",
    "            mergeScaled_df[col+suff] = transCol(mergeScaled_df[col+suff].values, col, cols2scale, scaler)\n",
    "mergeScaled_df.iloc[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyBt6Ch7Qv3D"
   },
   "source": [
    "### Form histories for each team for each game\n",
    "\n",
    "Given a game with a home team and an away team, retrieve the stats from the last N games for each team (who each team\n",
    "played and the stats of those games). If Ngames is np.inf, we'll store the season history for each team. This means the sequence size will be variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3qt9L8eCQv3D",
    "outputId": "54e2d813-dcf2-4f65-b8c8-0ffe977d551c"
   },
   "outputs": [],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KUzd9i_Qv3G",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we want the following:\n",
    "#for each game we want the past N game history for each team\n",
    "#need to divide up into teamIdx and seasIdx\n",
    "#we're going to store the information 'twice' to make code simpler\n",
    "team_col = 'TEAM_ID'\n",
    "sz_col = 'SZSTART'\n",
    "uteams = pd.unique(df_t[team_col])\n",
    "uszns = pd.unique(df_t[sz_col])\n",
    "team_dfs = np.zeros((len(uteams),len(uszns)), dtype=object)\n",
    "team2idx = {uteam: idx for idx, uteam in enumerate(uteams)}\n",
    "sz2idx = {uszn: idx for idx, uszn in enumerate(uszns)}\n",
    "common_cols = ['HOME','AWAY','SZSTART','PLAYOFF','REG','S','M','T','W','Th','F','Sa','DsinceSzn','MINUTES']\n",
    "              #'SEASON_ID','GAME_DATE']\n",
    "for teamIdx, team in enumerate(uteams):\n",
    "    teamMask = df_t[team_col] == team\n",
    "    for szIdx, sz in enumerate(uszns):\n",
    "        szMask = df_t[sz_col] == sz\n",
    "        team_mask = teamMask & szMask\n",
    "        non_team_mask = np.logical_not(teamMask) & szMask\n",
    "        tempNonTeam = clean_df[non_team_mask].copy()\n",
    "        tempTeam = clean_df[team_mask].copy()\n",
    "        merged = tempTeam.merge(tempNonTeam, 'inner',on='GAME_ID',suffixes=('_M','_S'))\n",
    "        merged = ridDupCols(merged,common_cols, '_S','_M', inplace=False)\n",
    "        team_dfs[teamIdx, szIdx] = merged #storing a view\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "n7dbInxHQv3K",
    "outputId": "17a818aa-6d49-4e7e-8ef8-357423d1fdc6"
   },
   "outputs": [],
   "source": [
    "\n",
    "merge_df[merge_df['GAME_ID']==20601230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "colab_type": "code",
    "id": "KF4D-587Qv3O",
    "outputId": "1e100ef5-5d0b-4bdf-8742-a3cb908b74da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minG = 20\n",
    "Ngames = 20\n",
    "#make Ngames inf to have \"rolling\" history\n",
    "teamColH = team_col + \"_H\"\n",
    "teamColA = team_col + \"_A\"\n",
    "gb4_col = 'GsinceSzn'\n",
    "gb4_colH =  gb4_col +\"_H\"\n",
    "gb4_colA =  gb4_col +\"_A\"\n",
    "\n",
    "sample_df = team_dfs[0,0]\n",
    "ind_cols = [n for n in numCols if n not in common_cols] + team_names.tolist()\n",
    "ind_cols = [c + suff for suff in ('_M','_S') for c in ind_cols]\n",
    "#convert these to int idxs\n",
    "commonColsI = [sample_df.columns.get_loc(c) for c in common_cols]\n",
    "indColsI = [sample_df.columns.get_loc(c) for c in ind_cols]\n",
    "colI = commonColsI + indColsI\n",
    "\n",
    "curr_cols = curr_commcols +  [c + suff for suff in ('_H','_A') for c in currIcols]\n",
    "\n",
    "minGmask =((merge_df[gb4_colH] > minG) &\n",
    "           (merge_df[gb4_colA] > minG))\n",
    "numSets = sum(minGmask)\n",
    "h_hists = [None]*numSets #Ngame histories for the home team of currentMatchup\n",
    "a_hists = [None]*numSets# \"\" away\n",
    "currMatchup = [None]*numSets #stats that would be known prior to the game (date played, G/DsinceSzn, etc.) [not teams bc that's contained in histories]\n",
    "ys = [None]*numSets #point differential of currMatchup\n",
    "count = 0\n",
    "SZNtracker = [None]*numSets\n",
    "for index, row in merge_df.loc[minGmask].iterrows():\n",
    "    szIdx = sz2idx[row[sz_col]]\n",
    "    SZNtracker[count] = row[sz_col]\n",
    "    teamIdxH = team2idx[row[teamColH]]\n",
    "    teamIdxA = team2idx[row[teamColA]]\n",
    "    h_df = team_dfs[teamIdxH, szIdx]\n",
    "    a_df = team_dfs[teamIdxA, szIdx]\n",
    "    \n",
    "    gb4_H = row[gb4_colH]\n",
    "    gb4_A = row[gb4_colA]\n",
    "    \n",
    "    diffHistL = gb4_H - gb4_A\n",
    "    stIdx_A = max(0, gb4_A - Ngames)\n",
    "    stIdx_H = max(0, gb4_H - Ngames)\n",
    "    if diffHistL < 0:\n",
    "        #away team has more history\n",
    "        stIdx_A = max(-diffHistL, gb4_A - Ngames)\n",
    "    elif diffHistL > 0:\n",
    "        stIdx_H = max(diffHistL, gb4_H - Ngames)\n",
    "    \n",
    "    h_hists[count] = h_df.iloc[stIdx_H:gb4_H, colI] \n",
    "    a_hists[count] = a_df.iloc[stIdx_A:gb4_A, colI]\n",
    "    \n",
    "    currMatchup[count] = mergeScaled_df.loc[index, curr_cols]\n",
    "    ys[count] = row['PTS_H'] - row['PTS_A']\n",
    "    count+=1 \n",
    "    if count % 1000 == 0:\n",
    "        print(\"{} / {}\".format(count, numSets))\n",
    "print(\"{} / {}\".format(count, numSets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "id": "mO9E4kS7Qv3R",
    "outputId": "8cbbee92-4155-4b5e-fce5-fc9c6ecb9694"
   },
   "outputs": [],
   "source": [
    "currMatchup[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "BqT_jJkMQv3V",
    "outputId": "137cc410-3945-4c08-8917-38fbfce2e44e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h_hists[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "ZrkyJuKgQv3Z",
    "outputId": "00428993-a072-4b4a-92a7-1b23a3fc7d25",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_hists[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "YArfXKjJQv3b",
    "outputId": "3c79df83-9b9b-4de0-b3a3-9bee92113145"
   },
   "outputs": [],
   "source": [
    "ys[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dHPNlPNMQv3g",
    "outputId": "48fe2da9-f35a-4642-f2d7-0a5382bd749d"
   },
   "outputs": [],
   "source": [
    "SZNtracker[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wT14CdiaN5oW",
    "outputId": "99cd9de0-55dd-4098-ff79-432645259a84"
   },
   "outputs": [],
   "source": [
    "numSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWdvpmG4Qv3i"
   },
   "outputs": [],
   "source": [
    "#now going to make everything numpy arrays and concatenate the home and away histories\n",
    "hist = [None]*numSets\n",
    "currMatchVals = [None]*numSets\n",
    "for i in range(numSets):\n",
    "    currMatchVals[i] = currMatchup[i].values\n",
    "    hist[i] = np.hstack((h_hists[i].values, a_hists[i].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2OF-k08Qv3k"
   },
   "source": [
    "# Start Pytorch\n",
    "\n",
    "We now have (home_hist, away_hist) and (home_points_scored - away_points_scored) as x and y. Home hist and away hist\n",
    "are views of the previous N games for each team, where N can be variable across sequences (but must be the same for home hist and away hist). \n",
    "\n",
    "The idea is that we will feed the concatenated histories into the RNN, concatenate the currMatchup with the output\n",
    "of the RNN and then feed that into a single layer to do the regression. The RNN will learn the best function to perform over the histories (a naive guess would be the average stats over the last N games is the best function to predict the outcome in the curr Matchup). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evIKWgvOQv3m"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#ok first have to define a dataset\n",
    "class NBADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Class to retreive y, currMatchup, history\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,matches_list, histories_list, ys_list, SZNtracker, device) :\n",
    "        \"\"\"\n",
    "        THEY SHOULD BE SORTED IN TIME; We also do not copy, so this data should be \n",
    "        considered read only\n",
    "        \n",
    "        maches_list[i] occured before matches_list[i+1]\n",
    "        matches_list - each element is a vector containing matchup info\n",
    "                        - day of week, days since season begain, playoff game etc. \n",
    "        histories_list - each element is a 2d array (Ngames, Nfeatures) which consists of \n",
    "                        the home team's and away team's stats over the last N games. \n",
    "        ys_list - each element is a point differential (home - away)\n",
    "        SZNtracker - list of seasons (starting year)\n",
    "        \n",
    "        For every game, the matches list has info on the game, the histories has info on how each team in the\n",
    "        game has performed over their last N games, the ys list has the outcome of the game, and the szntracker\n",
    "        knows in what season each game occurs.\n",
    "        \"\"\"\n",
    "        assert len(matches_list) == len(histories_list) == len(ys_list)== len(SZNtracker),\"all inputs must have same length!\"\n",
    "        self.len = len(matches_list)\n",
    "        self.device = device\n",
    "        self.match, self.hists, self.ys = self.tensorify(matches_list, \n",
    "                                                         histories_list, ys_list)\n",
    "        self.szns = np.array(SZNtracker)\n",
    "        \n",
    "    def tensorify(self,matches_list, histories_list, ys_list):\n",
    "        \"\"\"\n",
    "        Convert matches_list and ys_list to tensors. \n",
    "        Convert the elements of histories_list to tensors, but\n",
    "        hists will still be an array of tensors (bc uneven lengths)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            match = torch.as_tensor(np.array(matches_list,dtype=float, \n",
    "                                         copy=False)).float().to(device=self.device)\n",
    "            ys = torch.reshape(torch.as_tensor(ys_list).float().to(device=self.device),\n",
    "                           (-1,1))\n",
    "          #look at shapes, see if even or uneven lengths\n",
    "            numRows = np.fromiter((arr.shape[0] for arr in histories_list), dtype=int,\n",
    "                              count=self.len)\n",
    "            if (np.all(numRows[0] == numRows)):\n",
    "                #can just make this one giant tensor\n",
    "                hists = torch.as_tensor(histories_list).float().to(device=self.device)\n",
    "            else:\n",
    "                #cannot make one giant tensor\n",
    "                raise ValueError(\"wont work for now\")\n",
    "                #hists = np.array([torch.as_tensor(h).float().to(device=self.device) \n",
    "                #          for h in histories_list], dtype=object, copy=False)\n",
    "        except TypeError:\n",
    "                #we must be splitting and they're already tensors\n",
    "                match = matches_list\n",
    "                hists = histories_list\n",
    "                ys = ys_list\n",
    "        return match, hists, ys\n",
    "\n",
    "    def split(self, szn, frac):\n",
    "        \"\"\"\n",
    "        Split the dataset into thre more datasets:\n",
    "         1) the data that occurs before the point defined by \n",
    "         $frac % through season $szn. \n",
    "         2) The data that occurs after X % through each season after szn\n",
    "         3) the leftovers (the data that occurs before X % through each season after szn)\n",
    "        \n",
    "        This will allow us to split the data into training and testing sets. \n",
    "        Say we want to test on the latter halfs of seasons 2018, 2019 and 2020. \n",
    "        We call self.split(2018, .5). The first dataset will be training data prior\n",
    "        to 2018.5 . The second dataset will be the tets dataset (the second halfs of \n",
    "        2018, 2019, 20202), and the third dataset will be the first halfs of 2019, \n",
    "        and 2020. We can't train on this data and test on the later half of 2018 because\n",
    "        we'll be showing information form the future that it normally wouldn't have. \n",
    "        Theoretically we could test on this data, but there won't be as much history \n",
    "        built up. \n",
    "        \"\"\"\n",
    "        \n",
    "        assert frac >=0 and frac <= 1, \"frac must be [0,1]\"\n",
    "        \n",
    "        szns = np.unique(self.szns)\n",
    "        szns.sort()\n",
    "        szns2split = szns[szns >= szn]\n",
    "        if len(szns2split) == 0:\n",
    "            return None\n",
    "        firstIdxs = [None]*(len(szns2split)+1)\n",
    "        splitIdxs = [None]*len(szns2split)\n",
    "        for i, szn2sp in enumerate(szns2split):\n",
    "            inSznMask = self.szns == szn2sp\n",
    "            lenSzn = sum(inSznMask)\n",
    "            firstIdxs[i] = np.argmax(inSznMask)\n",
    "            splitIdxs[i] = firstIdxs[i] + int(frac*lenSzn)\n",
    "        firstIdxs[i+1] = self.len\n",
    "        tokeep = []\n",
    "        leftOver = []\n",
    "        for i in range(len(splitIdxs)):\n",
    "            start = firstIdxs[i]\n",
    "            split = splitIdxs[i]\n",
    "            end = firstIdxs[i+1]\n",
    "            tokeep.append(np.arange(split,end))\n",
    "            if i > 0:\n",
    "                leftOver.append(np.arange(start, split))\n",
    "        \n",
    "        keepIdx = np.concatenate(tokeep)\n",
    "        rmIdx = np.concatenate(leftOver)\n",
    "        #test dataset\n",
    "        dtest = NBADataset(self.match[keepIdx],\n",
    "                          self.hists[keepIdx],\n",
    "                            self.ys[keepIdx],\n",
    "                           self.szns[keepIdx],\n",
    "                           device)\n",
    "        dLeftOver = NBADataset(self.match[rmIdx],\n",
    "                              self.hists[rmIdx],\n",
    "                              self.ys[rmIdx],\n",
    "                              self.szns[rmIdx],\n",
    "                               device)\n",
    "        #training dataset\n",
    "        idx = splitIdxs[0]\n",
    "        dtrain = NBADataset(self.match[:idx],\n",
    "                           self.hists[:idx], \n",
    "                           self.ys[:idx], \n",
    "                           self.szns[:idx],\n",
    "                            device)\n",
    "        return dtrain, dtest, dLeftOver\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "         return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.match[idx, :], self.hists[idx], self.ys[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BzFc3QOGQv3n"
   },
   "outputs": [],
   "source": [
    "#now for the model\n",
    "#Now for the model \n",
    "import torch.nn as nn\n",
    "class RNNregressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,output_dim,currFeats,\n",
    "                                     n_layers=1, dropP=0,batch_first=False):\n",
    "        super(RNNregressor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_first = batch_first\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers,dropout=dropP,\n",
    "                          batch_first=batch_first)\n",
    "        self.dense = nn.Linear(hidden_dim+currFeats, output_dim)\n",
    "        #self.dense = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        if self.batch_first:\n",
    "            self.indexFN = self.indexBatchFirst\n",
    "        else:\n",
    "            self.indexFN = self.indexSeqFirst\n",
    "    \n",
    "    def indexBatchFirst(self,out):\n",
    "        return out[:,-1,:]\n",
    "    def indexSeqFirst(self, out):\n",
    "        return out[-1,:,:]\n",
    "    \n",
    "    def forward(self, x, curr):\n",
    "        out, _ = self.gru(x) #out will be [N, L, output_dim]\n",
    "        outVec = self.indexFN(out)\n",
    "        l_in = torch.cat((curr, outVec),axis=1) #last sequence, last layer, all in batch\n",
    "        return self.dense(self.relu(l_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbJ8T_M8Qv3q"
   },
   "outputs": [],
   "source": [
    "def train(loader, model,loss_fn,opt, epochs):\n",
    "    losses = [None]*len(loader)*epochs\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for curr,hist,y in loader:\n",
    "            yhat = model(hist, curr)\n",
    "            loss = loss_fn(yhat, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            losses[count] = loss.item()\n",
    "            count+=1 \n",
    "        print(\"epoch\", epoch)\n",
    "    return model, losses\n",
    "\n",
    "def test(loader, model, loss_fn):\n",
    "    losses = [None]*len(loader)\n",
    "    running_right = 0\n",
    "    running_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (curr, hist, y) in enumerate(loader):\n",
    "            yhat = model(hist, curr)\n",
    "            losses[i] = loss_fn(yhat, y).item()\n",
    "            yhatnp = yhat.cpu().numpy()\n",
    "            ynp = y.cpu().numpy()\n",
    "            correctMask = ynp*yhatnp > 0\n",
    "            running_right = sum(correctMask)\n",
    "            running_total = len(correctMask)\n",
    "        return losses, running_right/running_total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PYexuvZiQv3t"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "num_workers=0\n",
    "dataset = NBADataset(currMatchVals, hist,ys, SZNtracker, device)\n",
    "#dtrain, dtest, dleft = dataset.split(2017, .5)\n",
    "train_per = 80\n",
    "test_per = 20\n",
    "train_len = int(train_per/100.0*count)\n",
    "\n",
    "test_len =count - train_len\n",
    "torch.manual_seed(27)\n",
    "\n",
    "dtrain, dtest= random_split(dataset,[train_len, test_len]) \n",
    "train_load = DataLoader(dtrain, batch_size=20, num_workers=num_workers)\n",
    "test_load = DataLoader(dtest, batch_size=20,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hXmh9dIQv3v"
   },
   "outputs": [],
   "source": [
    "input_dim = 256\n",
    "hidden_dim = 100\n",
    "output_dim = 1\n",
    "currMatchup_dim = 13\n",
    "n_layers = 1\n",
    "dropP = 0\n",
    "batchFirst = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "colab_type": "code",
    "id": "ma9xLe43Qv3x",
    "outputId": "01eccf53-70d8-4111-be3e-dfa3c18192ea"
   },
   "outputs": [],
   "source": [
    "epochs = 13\n",
    "lr = .005\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "wl_accs = []\n",
    "\n",
    "rnn = RNNregressor(input_dim, hidden_dim, output_dim, currMatchup_dim,\n",
    "                  n_layers, dropP, batchFirst)\n",
    "rnn.to(device=device)\n",
    "opt = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(epochs):\n",
    "    rnn, losses = train(train_load, rnn, loss_fn, opt, 1)\n",
    "    test_loss, test_wl_acc = test(test_load, rnn, loss_fn)\n",
    "    train_losses.append(np.mean(losses))\n",
    "    test_losses.append(np.mean(test_loss))\n",
    "    wl_accs.append(test_wl_acc)\n",
    "    print(\"train/testloss | acc: {} / {} | {}\".format(np.mean(losses), np.mean(test_loss), test_wl_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "Wajz4N8qqHqO",
    "outputId": "31fe61e2-dc65-4b8a-91d3-5b884893b8ef"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fs= 16\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ep = np.arange(len(train_losses))+1\n",
    "ax.plot(ep, train_losses, label='train')\n",
    "ax.plot(ep, test_losses, label=\"test\")\n",
    "ax.legend(fontsize=14)\n",
    "ax.axvline(8, linestyle='--', alpha=.5,color='k') #0 indexed\n",
    "ax.set_xlabel(\"Epoch\", fontsize=fs)\n",
    "ax.set_ylabel(\"Loss\", fontsize=fs)\n",
    "ax.set_title(\"Loss vs. Training Epoch\", fontsize=fs)\n",
    "fig.savefig(\"train_test.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "dbqDALiAuZ2s",
    "outputId": "2f3e135f-817d-4f0b-b0d0-41522076950c"
   },
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "acs = np.array(wl_accs)*np.max(train_losses)\n",
    "ax2.plot(ep, wl_accs)\n",
    "ax2.set_ylim([0,1])\n",
    "ax2.set_xlabel(\"Epoch\", fontsize=fs)\n",
    "ax2.set_ylabel(\"W/L Accuracy\", fontsize=fs)\n",
    "ax2.set_title(\"W/L Accuracy vs. Training Epoch\", fontsize=fs)\n",
    "ax2.axvline(8, linestyle='--', alpha=.5, color='k')\n",
    "fig2.savefig(\"wl_acc.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kDoB8vt0xkH7",
    "outputId": "1de6e9a0-6ad4-4d6f-eb41-c9847dc400f2"
   },
   "outputs": [],
   "source": [
    "#figure out where the images are saved in google Colab\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVWICMs3xxsA"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    files.download('train_test.png')\n",
    "    files.download('wl_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMGJ_lGNx394"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "rnn3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
